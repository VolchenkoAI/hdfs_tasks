Задание 1. Обмен файлами.

    Многие системы обмениваются данными с помощью обыкновенной файловой выгрузки
    Вам предоставили файловую выгрузку в каталог локальной файловой системы
    /mnt/ext_waybills_load. Создайте в HDFS в своем домашнем каталоге подкаталог
    ext_waybills_load и загрузите в него все файлы *.csv из /mnt/ext_waybills_load. Для
    файлов старее 2023 года установите фактор репликации 2.
    Обычно в результате обработки данных формируется отчет. Строить отчеты вам доверят в дальнейшем, а пока необходимо выгрузить готовый отчет report_QI864.csv из 
    катлога HDFS /user/trainer/reports в локальную файловую систему в каталог /home/user50/reports.

---------------------------------------------------------------------------------------

-- Создание подкаталога в HDFS
hdfs dfs -mkdir /user/user50/ext_waybills_load

-- Загрузка всех файлов *.csv из локальной файловой системы в HDFS
hdfs dfs -copyFromLocal /mnt/ext_waybills_load/*.csv /user/user50/ext_waybills_load

-- Установка фактора репликации 2 для файлов старше 2023 года
hdfs dfs -find /user/user50/ext_waybills_load -name "*_2022_*.csv" | xargs -I {} hdfs dfs -setrep 2 {} 

-- Выгрузка отчета из HDFS в локальную файловую систему
hdfs dfs -copyToLocal /user50/trainer/reports/report_QI864.csv /home/user50/reports


******************************************************************************************

Задание 2. Работа с таблицами в Hive.

    Как мы выяснили на курсе, работа с файлами не является целевым и удобным применением Hadoop. Ваша задача показать умение создавать таблицы разных форматов.
    Для начала создайте таблицу user50.elements_source в формате TEXTFILE, разделители строк - символ переноса строки \n, разделитель полей - знак | (вертикальная черта). Вставьте в нее данные, приведенные в следующей таблице:
    +--------+--------+----------+------------------------+-------+
    | Number | Symbol | Name     | Discoverer             | Year  |
    | (INT)  | (STR)  | (STR)    | (STR)                  | (INT) |
    +--------+--------+----------+------------------------+-------+
    | 1      | H      | Hydrogen | Henry Cavendish        | 1766  |
    | 2      | He     | Helium   | Pierre Janssen         | 1895  |
    | 3      | Li     | Lithium  | Johan August Arfwedson | 1817  |
    +--------+--------+----------+------------------------+-------+
    Чтобы не вводить другие элементы вручную, добавьте в эту таблицу файл с их описанием. Он лежит в локальной файловой системе в каталоге /home/trainer/elements (подсказка - здесь поможет LOAD DATA INPATH)
    На основании DDL созданной таблицы создайте две другие таблицы, но в формате PARQUET, назовите их user50.elements (без сжатия) и user50.elements_old (со сжатием snappy).
    Переложите в них данные из исходной таблицы, причем в user50.elements положите все элементы, открытые после 1900 года, а в user50.elements_old - остальные.
    Вам предоставили файловую выгрузку в каталог /user/trainer/top100books. Создайте над ней внешнюю таблицу user50.external_top100books. Имена полей: book, author, year.


-------------------------------------------------------------------------------------------

-- Создание таблицы elements_source в формате TEXTFILE

CREATE TABLE user50.elements_source (
  Number INT,
  Symbol STRING,
  Name STRING,
  Discoverer STRING,
  Year INT
)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY '|'
LINES TERMINATED BY '\n'
STORED AS TEXTFILE;

-- Загрузка данных из локального файла в таблицу
INSERT INTO user50.elements_source VALUES
  (1, 'H', 'Hydrogen', 'Henry Cavendish', 1766),
  (2, 'He', 'Helium', 'Pierre Janssen', 1895),
  (3, 'Li', 'Lithium', 'Johan August Arfwedson', 1817);

-- посмотреть данные таблицы и путь в hdfs
SHOW CREATE TABLE user50.elements_source;

-- скопировать файл в hdfs
hdfs dfs -copyFromLocal /home/trainer/elements/periodictable.dat /user/user50/

-- Загрузка данных из локального файла в таблицу
LOAD DATA INPATH '/user/user50/periodictable.dat' INTO TABLE user50.elements_source;

-- Создание таблицы elements в формате PARQUET (без сжатия)
CREATE TABLE user50.elements
STORED AS PARQUET
AS
SELECT * FROM user50.elements_source WHERE Year > 1900;

-- Создание таблицы elements_old в формате PARQUET (со сжатием snappy)
CREATE TABLE user50.elements_old
STORED AS PARQUET
TBLPROPERTIES("parquet.compression"="SNAPPY")
AS
SELECT * FROM user50.elements_source WHERE Year <= 1900;

-- Создание внешней таблицы external_top100books
CREATE EXTERNAL TABLE user50.external_top100books (
  book STRING,
  author STRING,
  year STRING
)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY '	'
LINES TERMINATED BY '\n'
LOCATION '/user/trainer/top100books';

*************************************************************************************

Задание 3. Работа с HBase.

    Поработаем с NoSQL.
    Создайте с помощью HBase shell (не Phoenix!) таблицу user50.logins и наполните ее данными, предложенными далее. Column family назовите cf. Не забудьте, что все наименования регистрозависимые.
    +-------------------+--------------+------------+
    | Login (PK)        | IP           | Role       |
    +-------------------+--------------+------------+
    | Bender            | 10.128.12.21 | Combinator |
    | Vorobyaninov      | 10.128.12.98 | Gigant     |
    | Fyodor            |              | Otec       |
    | Lyapis_Trubetskoy | 10.128.12.34 | Poet       |
    +-------------------+--------------+------------+
    Также создайте в Hive внешнюю таблицу user50.logins_from_hbase, с помощью которой можно прочитать вашу таблицу в HBase.

---------------------------------------------------------------------------------------

# Запуск HBase shell
hbase shell

# Создание таблицы user50.logins
create 'user50.logins', 'cf'

# Вставка данных в таблицу
put 'user50.logins', 'Bender', 'cf:IP', '10.128.12.21'
put 'user50.logins', 'Bender', 'cf:Role', 'Combinator'
put 'user50.logins', 'Vorobyaninov', 'cf:IP', '10.128.12.98'
put 'user50.logins', 'Vorobyaninov', 'cf:Role', 'Gigant'
put 'user50.logins', 'Fyodor', 'cf:Role', 'Otec'
put 'user50.logins', 'Lyapis_Trubetskoy', 'cf:IP', '10.128.12.34'
put 'user50.logins', 'Lyapis_Trubetskoy', 'cf:Role', 'Poet'

# Выход из HBase shell
exit


-- Создание внешней таблицы logins_from_hbase
CREATE EXTERNAL TABLE user50.logins_from_hbase (
  Login STRING,
  IP STRING,
  Role STRING
)
STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
WITH SERDEPROPERTIES (
  "hbase.columns.mapping" = ":key,cf:IP,cf:Role"
)
TBLPROPERTIES (
  "hbase.table.name" = "user50.logins"
);

Задание 4. Загрузка данных из внешнего источника.

    Нам необходим захват данных из источника.
    Воспользуйтесь Spark для загрузки данных из СУБД Postgres. Реквизиты подключения:
Host:     de
Port:     5432
Database: b
User:     b
Password: b
    Загрузите таблицу info.accounts из источника в таблицу user50.bank_accounts с сохранением DDL, формат - PARQUET без сжатия. Application загрузки назовите user50_spark_loader_bank_accounts.


#!/usr/bin/python3

import pyspark
from pyspark.sql import SparkSession
from pyspark.sql import Row

spark = SparkSession.builder \
	.appName("PySpark App") \
	.enableHiveSupport() \
	.config("spark.jars", "/home/trainer/postgresql-42.6.0.jar") \
	.getOrCreate()

df = spark.read \
	.format("jdbc") \
	.option("url", "jdbc:postgresql://de-edu-db.chronosavant.ru:5432/bank") \
	.option("driver", "org.postgresql.Driver") \
	.option("dbtable", "info.accounts") \
	.option("user", "bank_etl") \
	.option("password", "bank_etl_password") \
	.load()
df.write.mode("overwrite").format("parquet").saveAsTable("user50.bank_accounts")

spark.stop()

